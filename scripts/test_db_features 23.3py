#!/usr/bin/env python
# scripts/test_db_features.py

import os
import sys
import time
import logging
import argparse
import re
import json
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"db_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
        logging.StreamHandler(sys.stdout)  # Print to console
    ]
)
logger = logging.getLogger("DB_TESTS")

# Print a starting message to confirm the script is running
print("Starting enhanced database test script (fixed transaction handling)...")

# Define test result tracking
class TestResults:
    def __init__(self):
        self.passed = 0
        self.failed = 0
        self.skipped = 0
        self.failures = []
    
    def add_pass(self, test_name=None):
        self.passed += 1
        if test_name:
            logger.info(f"PASS: {test_name}")
    
    def add_fail(self, test_name, error_msg):
        self.failed += 1
        self.failures.append((test_name, error_msg))
        logger.error(f"FAIL: {test_name} - {error_msg}")
    
    def add_skip(self, test_name=None):
        self.skipped += 1
        if test_name:
            logger.info(f"SKIP: {test_name}")
    
    def summary(self):
        return (f"\n======== TEST SUMMARY ========\n"
                f"PASSED:  {self.passed}\n"
                f"FAILED:  {self.failed}\n"
                f"SKIPPED: {self.skipped}\n"
                f"============================\n")
    
    def failures_summary(self):
        if not self.failures:
            return "No failures detected."
        
        result = "\n======= FAILURES DETAIL =======\n"
        for i, (test, error) in enumerate(self.failures, 1):
            result += f"{i}. {test}: {error}\n"
        result += "============================\n"
        return result

# Initialize test results
results = TestResults()

def print_section_header(title):
    """Print a section header to make test output more readable"""
    section_width = 60
    padding = (section_width - len(title) - 2) // 2
    logger.info("\n" + "=" * section_width)
    logger.info(" " * padding + title + " " * padding)
    logger.info("=" * section_width)

def read_env_file():
    """Read database URLs from .env file"""
    db_urls = {}
    
    # Try to read from .env file
    env_path = Path('.env')
    if env_path.exists():
        logger.info("Reading database URLs from .env file")
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    if '=' in line:
                        key, value = line.split('=', 1)
                        if key in ('FLASK_ENV', 'DEV_DATABASE_URL', 'TEST_DATABASE_URL', 'PROD_DATABASE_URL'):
                            db_urls[key] = value
                            os.environ[key] = value  # Also set as environment variable
    
    # Set some default values if not found
    if 'DEV_DATABASE_URL' not in db_urls:
        db_urls['DEV_DATABASE_URL'] = 'postgresql://skinspire_admin:Skinspire123$@localhost:5432/skinspire_dev'
        os.environ['DEV_DATABASE_URL'] = db_urls['DEV_DATABASE_URL']
    if 'TEST_DATABASE_URL' not in db_urls:
        db_urls['TEST_DATABASE_URL'] = 'postgresql://skinspire_admin:Skinspire123$@localhost:5432/skinspire_test'
        os.environ['TEST_DATABASE_URL'] = db_urls['TEST_DATABASE_URL']
    
    logger.info(f"Found database URLs:")
    for key in db_urls:
        if key.endswith('DATABASE_URL'):
            masked_url = re.sub(r'://([^:]+):([^@]+)@', r'://\1:***@', db_urls[key])
            logger.info(f"  {key}: {masked_url}")
        else:
            logger.info(f"  {key}: {db_urls[key]}")
    
    return db_urls

def get_db_url(env):
    """Get database URL for specified environment"""
    env_key = f"{env.upper()}_DATABASE_URL"
    return os.environ.get(env_key)

def get_db_engine(env):
    """Get SQLAlchemy engine for specified environment"""
    from sqlalchemy import create_engine
    
    db_url = get_db_url(env)
    if not db_url:
        raise ValueError(f"No database URL found for environment: {env}")
    
    return create_engine(db_url)

def execute_sql(env, sql, params=None):
    """Execute SQL statement and return result"""
    from sqlalchemy import create_engine, text
    
    engine = get_db_engine(env)
    with engine.connect() as connection:
        # Create a fresh transaction for this connection
        with connection.begin():
            if params:
                result = connection.execute(text(sql), params)
            else:
                result = connection.execute(text(sql))
            return result

def test_database_connection(env):
    """Test database connection using SQLAlchemy"""
    db_url = get_db_url(env)
    if not db_url:
        logger.error(f"No database URL found for environment: {env}")
        return False
        
    masked_url = re.sub(r'://([^:]+):([^@]+)@', r'://\1:***@', db_url)
    logger.info(f"Testing connection to database: {masked_url}")
    
    try:
        # Import SQLAlchemy
        from sqlalchemy import create_engine, text
        
        # Create engine
        engine = get_db_engine(env)
        
        # Test connection with a simple query
        with engine.connect() as connection:
            # Create a transaction to ensure proper cleanup
            with connection.begin():
                result = connection.execute(text("SELECT 1 as test_connection"))
                row = result.fetchone()
                if row and row[0] == 1:
                    logger.info(f"Database connection successful for {env} environment")
                    return True
                else:
                    logger.error(f"Database connection failed for {env} environment - unexpected result")
                    return False
    except ImportError:
        logger.error("SQLAlchemy not installed. Please install it with 'pip install sqlalchemy'")
        return False
    except Exception as e:
        logger.error(f"Database connection failed for {env} environment: {e}")
        return False

def get_table_names(env):
    """Get list of tables in database"""
    try:
        from sqlalchemy import create_engine, inspect, text
        
        engine = get_db_engine(env)
        inspector = inspect(engine)
        
        # Get all schemas
        schemas = inspector.get_schema_names()
        
        # Get tables for each schema
        all_tables = []
        for schema in schemas:
            if schema != 'information_schema' and not schema.startswith('pg_'):
                schema_tables = inspector.get_table_names(schema=schema)
                all_tables.extend([f"{schema}.{table}" if schema != 'public' else table 
                                for table in schema_tables])
        
        return all_tables
    except Exception as e:
        logger.error(f"Error getting table names for {env}: {e}")
        return []

def backup_database(env):
    """Create database backup using SQLAlchemy"""
    print_section_header(f"BACKING UP {env.upper()} DATABASE")
    
    try:
        from sqlalchemy import create_engine, text, MetaData, Table, inspect
        import sqlalchemy.schema
        
        engine = get_db_engine(env)
        metadata = MetaData()
        inspector = inspect(engine)
        
        backup_data = {
            'tables': {},
            'sequences': {},
            'timestamp': datetime.now().isoformat()
        }
        
        # Get all tables
        tables = get_table_names(env)
        logger.info(f"Found {len(tables)} tables in {env} database")
        
        # Backup each table structure and data
        with engine.connect() as connection:
            # Create a single transaction for all operations
            with connection.begin():
                for table_name in tables:
                    # Skip system tables
                    if table_name.startswith('pg_') or table_name == 'alembic_version':
                        continue
                        
                    logger.info(f"Backing up table: {table_name}")
                    
                    # Get table structure
                    if '.' in table_name:
                        schema, name = table_name.split('.', 1)
                    else:
                        schema, name = 'public', table_name
                    
                    columns = inspector.get_columns(name, schema=schema)
                    primary_keys = inspector.get_pk_constraint(name, schema=schema)
                    foreign_keys = inspector.get_foreign_keys(name, schema=schema)
                    indices = inspector.get_indexes(name, schema=schema)
                    
                    # Get table data
                    result = connection.execute(text(f'SELECT * FROM {table_name}'))
                    rows = [dict(row._mapping) for row in result]
                    
                    # Store table information
                    backup_data['tables'][table_name] = {
                        'columns': columns,
                        'primary_keys': primary_keys,
                        'foreign_keys': foreign_keys,
                        'indices': indices,
                        'rows': rows
                    }
                
                # Get sequence information
                seq_query = text("""
                    SELECT c.relname as sequence_name
                    FROM pg_class c
                    WHERE c.relkind = 'S'
                    AND c.relnamespace IN (
                        SELECT oid FROM pg_namespace 
                        WHERE nspname NOT LIKE 'pg_%' AND nspname != 'information_schema'
                    )
                """)
                
                sequences = connection.execute(seq_query)
                for seq in sequences:
                    seq_name = seq.sequence_name
                    val_query = text(f"SELECT last_value FROM {seq_name}")
                    val_result = connection.execute(val_query)
                    last_value = val_result.scalar()
                    
                    backup_data['sequences'][seq_name] = last_value
        
        # Save backup to file
        backup_dir = Path('backups')
        if not backup_dir.exists():
            backup_dir.mkdir(parents=True)
            
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = backup_dir / f"{env}_backup_{timestamp}.json"
        
        with open(backup_file, 'w') as f:
            json.dump(backup_data, f, default=str)
            
        logger.info(f"Backup completed: {backup_file}")
        results.add_pass(f"Backup {env} Database")
        return backup_file
        
    except Exception as e:
        logger.error(f"Error backing up {env} database: {e}")
        results.add_fail(f"Backup {env} Database", str(e))
        return None

def restore_database(env, backup_file):
    """Restore database from backup using SQLAlchemy"""
    print_section_header(f"RESTORING {env.upper()} DATABASE")
    
    try:
        from sqlalchemy import create_engine, text
        
        if not backup_file.exists():
            raise FileNotFoundError(f"Backup file not found: {backup_file}")
            
        # Load backup data
        with open(backup_file, 'r') as f:
            backup_data = json.load(f)
            
        engine = get_db_engine(env)
        
        # Drop all tables first (outside transaction to avoid lock issues)
        with engine.connect() as connection:
            # Create and commit a transaction just for this operation
            with connection.begin():
                # Disable foreign key checks
                connection.execute(text("SET session_replication_role = 'replica';"))
                
                # Get existing tables
                existing_tables = get_table_names(env)
                
                # Build DROP TABLE statement for all tables
                if existing_tables:
                    drop_tables_sql = f"DROP TABLE IF EXISTS {', '.join(existing_tables)} CASCADE;"
                    connection.execute(text(drop_tables_sql))
                    logger.info(f"Dropped {len(existing_tables)} existing tables")
        
        # Now recreate tables and restore data in a separate transaction
        with engine.connect() as connection:
            with connection.begin():
                # Recreate tables and restore data
                for table_name, table_data in backup_data['tables'].items():
                    logger.info(f"Restoring table: {table_name}")
                    
                    # Create table dynamically based on backup information
                    columns_info = table_data['columns']
                    primary_keys = table_data['primary_keys']['constrained_columns']
                    
                    # Build CREATE TABLE statement
                    column_defs = []
                    for col in columns_info:
                        col_type = col['type']
                        nullable = "" if col.get('nullable', True) else "NOT NULL"
                        column_defs.append(f"{col['name']} {col_type} {nullable}")
                    
                    if primary_keys:
                        column_defs.append(f"PRIMARY KEY ({', '.join(primary_keys)})")
                    
                    create_table_sql = f"CREATE TABLE {table_name} ({', '.join(column_defs)});"
                    connection.execute(text(create_table_sql))
                
                # Insert data in a separate transaction to avoid locks
                for table_name, table_data in backup_data['tables'].items():
                    if table_data['rows']:
                        logger.info(f"Inserting data into {table_name}")
                        
                        # Insert data in smaller batches to avoid transaction issues
                        batch_size = 100
                        for i in range(0, len(table_data['rows']), batch_size):
                            batch = table_data['rows'][i:i+batch_size]
                            
                            for row in batch:
                                # Build INSERT statement
                                columns = list(row.keys())
                                placeholders = [f":{col}" for col in columns]
                                
                                insert_sql = f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(placeholders)})"
                                connection.execute(text(insert_sql), row)
                
                # Restore sequences
                for seq_name, last_value in backup_data['sequences'].items():
                    connection.execute(text(f"SELECT setval('{seq_name}', {last_value}, true)"))
                
                # Re-enable foreign key checks
                connection.execute(text("SET session_replication_role = 'origin';"))
        
        logger.info(f"Successfully restored {env} database from {backup_file}")
        results.add_pass(f"Restore {env} Database")
        return True
                
    except Exception as e:
        logger.error(f"Error restoring {env} database: {e}")
        results.add_fail(f"Restore {env} Database", str(e))
        return False

def test_configuration():
    """Test configuration and database connectivity"""
    print_section_header("TESTING CONFIGURATION")
    
    # Read database URLs from .env file
    db_urls = read_env_file()
    
    # Test dev database connection
    dev_connected = False
    test_connected = False
    
    if 'DEV_DATABASE_URL' in db_urls:
        dev_connected = test_database_connection('dev')
        if dev_connected:
            results.add_pass("Configuration - Dev Database Connection")
        else:
            results.add_fail("Configuration - Dev Database Connection", "Failed to connect to dev database")
    else:
        logger.warning("DEV_DATABASE_URL not found")
        results.add_skip("Configuration - Dev Database Connection")
    
    # Test test database connection
    if 'TEST_DATABASE_URL' in db_urls:
        test_connected = test_database_connection('test')
        if test_connected:
            results.add_pass("Configuration - Test Database Connection")
        else:
            results.add_fail("Configuration - Test Database Connection", "Failed to connect to test database")
    else:
        logger.warning("TEST_DATABASE_URL not found")
        results.add_skip("Configuration - Test Database Connection")
    
    return dev_connected, test_connected

def test_backup_restore():
    """Test backup and restore functionality"""
    print_section_header("TESTING BACKUP AND RESTORE")
    
    # Ensure test database is accessible
    if not test_database_connection('test'):
        logger.error("Cannot access test database, skipping backup/restore tests")
        results.add_skip("Backup/Restore Tests")
        return
    
    try:
        from sqlalchemy import create_engine, text
        
        engine = get_db_engine('test')
        
        # Create a test table
        with engine.connect() as connection:
            # Create a transaction for all operations in this connection
            with connection.begin():
                # First check if the table already exists and drop it if it does
                connection.execute(text("DROP TABLE IF EXISTS _test_backup_table"))
                
                # Create the test table
                connection.execute(text("""
                    CREATE TABLE _test_trigger_table (
                        id SERIAL PRIMARY KEY,
                        name VARCHAR(100),
                        created_at TIMESTAMP,
                        updated_at TIMESTAMP
                    )
                """))
                results.add_pass("Triggers - Create Test Table")
                
                # Check if update_timestamp function already exists
                # We need to be careful here because this function might be used by existing tables
                result = connection.execute(text("""
                    SELECT 1 FROM pg_proc p
                    JOIN pg_namespace n ON p.pronamespace = n.oid
                    WHERE p.proname = 'update_timestamp' AND n.nspname = 'public'
                """))
                
                function_exists = result.scalar() is not None
                
                # If function exists, we'll use it; otherwise create a test-specific one
                if not function_exists:
                    # Create a test-specific trigger function with a unique name
                    connection.execute(text("""
                        CREATE OR REPLACE FUNCTION test_update_timestamp()
                        RETURNS TRIGGER AS $
                        BEGIN
                            NEW.updated_at = NOW();
                            IF TG_OP = 'INSERT' THEN
                                NEW.created_at = NOW();
                            END IF;
                            RETURN NEW;
                        END;
                        $ LANGUAGE plpgsql;
                    """))
                    trigger_func_name = "test_update_timestamp"
                else:
                    trigger_func_name = "update_timestamp"
                    
                results.add_pass("Triggers - Use/Create Trigger Function")
                
                # Create trigger
                connection.execute(text(f"""
                    CREATE TRIGGER update_timestamp
                    BEFORE INSERT OR UPDATE ON _test_trigger_table
                    FOR EACH ROW
                    EXECUTE FUNCTION {trigger_func_name}();
                """))
                results.add_pass("Triggers - Create Trigger")
                
                # Insert test data
                connection.execute(text("INSERT INTO _test_trigger_table (name) VALUES ('test_record')"))
                results.add_pass("Triggers - Insert Test Data")
                
                # Verify created_at was set
                result = connection.execute(text("SELECT created_at FROM _test_trigger_table WHERE name = 'test_record'"))
                timestamp = result.scalar()
                
                if timestamp:
                    logger.info(f"Timestamp was set correctly: {timestamp}")
                    results.add_pass("Triggers - Verify Created Timestamp")
                else:
                    results.add_fail("Triggers - Verify Created Timestamp", "Timestamp was not set")
                
                # Update record
                connection.execute(text("UPDATE _test_trigger_table SET name = 'updated_record'"))
                results.add_pass("Triggers - Update Test Data")
                
                # Verify updated_at was set
                result = connection.execute(text("SELECT updated_at FROM _test_trigger_table WHERE name = 'updated_record'"))
                timestamp = result.scalar()
                
                if timestamp:
                    logger.info(f"Updated timestamp was set correctly: {timestamp}")
                    results.add_pass("Triggers - Verify Updated Timestamp")
                else:
                    results.add_fail("Triggers - Verify Updated Timestamp", "Updated timestamp was not set")
                
                # Clean up - drop our test table
                connection.execute(text("DROP TABLE IF EXISTS _test_trigger_table"))
                
                # If we created a test-specific function, drop it
                if not function_exists:
                    connection.execute(text("DROP FUNCTION IF EXISTS test_update_timestamp()"))
                
                logger.info("Cleaned up test table and trigger function")
        
        # Restore test database to original state
        logger.info("Restoring test database to original state")
        restore_database('test', test_backup)
        
    except Exception as e:
        logger.error(f"Error in trigger management test: {e}")
        results.add_fail("Trigger Management Test", str(e))
        
        # Restore test database to original state even if test failed
        logger.info("Attempting to restore test database after error")
        restore_database('test', test_backup)

def test_backward_compatibility():
    """Test backward compatibility"""
    print_section_header("TESTING BACKWARD COMPATIBILITY")
    
    # This is a simplified test since we're using SQLAlchemy directly instead of the application's database_service
    
    # Ensure both databases are accessible
    dev_connected = test_database_connection('dev')
    test_connected = test_database_connection('test')
    
    if not (dev_connected or test_connected):
        logger.error("Cannot access either database, skipping backward compatibility tests")
        results.add_skip("Backward Compatibility Tests")
        return
    
    try:
        from sqlalchemy import create_engine, text
        
        # Check if databases have expected tables
        expected_tables = ['users', 'roles', 'hospitals', 'patients', 'staff']
        env = 'test' if test_connected else 'dev'
        
        engine = get_db_engine(env)
        with engine.connect() as connection:
            with connection.begin():
                # Check for expected tables
                tables_found = []
                for table in expected_tables:
                    check_query = text(f"""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_name = '{table}'
                        )
                    """)
                    result = connection.execute(check_query)
                    exists = result.scalar()
                    if exists:
                        tables_found.append(table)
                
                if tables_found:
                    logger.info(f"Found expected tables: {', '.join(tables_found)}")
                    results.add_pass("Backward Compatibility - Expected Tables")
                else:
                    logger.warning("No expected tables found. This may be normal for a fresh installation.")
                    results.add_skip("Backward Compatibility - Expected Tables")
                
                # Check for trigger functions
                func_query = text("""
                    SELECT proname
                    FROM pg_proc p
                    JOIN pg_namespace n ON p.pronamespace = n.oid
                    WHERE n.nspname = 'public'
                      AND p.proname LIKE '%timestamp%'
                """)
                result = connection.execute(func_query)
                trigger_funcs = [row[0] for row in result]
                
                if trigger_funcs:
                    logger.info(f"Found trigger functions: {', '.join(trigger_funcs)}")
                    results.add_pass("Backward Compatibility - Trigger Functions")
                else:
                    logger.warning("No timestamp trigger functions found.")
                    results.add_skip("Backward Compatibility - Trigger Functions")
        
    except Exception as e:
        logger.error(f"Error in backward compatibility test: {e}")
        results.add_fail("Backward Compatibility Test", str(e))

def clean_up_test_artifacts():
    """Clean up any test artifacts left behind"""
    print_section_header("CLEANING UP TEST ARTIFACTS")
    
    try:
        from sqlalchemy import create_engine, text
        
        # Define test tables to clean up
        test_tables = [
            "_test_backup_table",
            "_test_copy_table", 
            "_test_trigger_table",
            "_test_migration_table"
        ]
        
        # Clean up in both environments
        for env in ['dev', 'test']:
            if test_database_connection(env):
                engine = get_db_engine(env)
                with engine.connect() as connection:
                    # Use a single transaction for all cleanup operations
                    with connection.begin():
                        for table in test_tables:
                            try:
                                connection.execute(text(f"DROP TABLE IF EXISTS {table}"))
                                logger.info(f"Dropped table {table} in {env} (if it existed)")
                                results.add_pass(f"Cleanup - Remove {table} from {env}")
                            except Exception as e:
                                logger.warning(f"Error dropping table {table} in {env}: {e}")
                                results.add_fail(f"Cleanup - Remove {table} from {env}", str(e))
        
    except Exception as e:
        logger.error(f"Error in cleanup: {e}")
        results.add_fail("Cleanup", str(e))

def main():
    """Main test runner function"""
    parser = argparse.ArgumentParser(description='Test database features using SQLAlchemy')
    parser.add_argument('--all', action='store_true', help='Run all tests')
    parser.add_argument('--config', action='store_true', help='Test configuration')
    parser.add_argument('--backup', action='store_true', help='Test backup and restore functionality')
    parser.add_argument('--copy', action='store_true', help='Test database copy functionality')
    parser.add_argument('--triggers', action='store_true', help='Test trigger management')
    parser.add_argument('--compat', action='store_true', help='Test backward compatibility')
    parser.add_argument('--skip-cleanup', action='store_true', help='Skip cleanup step')
    parser.add_argument('--no-env-preserve', action='store_true', 
                      help='Skip environment preservation (by default, environments are backed up and restored)')
    
    args = parser.parse_args()
    
    # If no specific tests are selected, run all tests
    run_all = args.all or not (args.config or args.backup or args.copy or 
                             args.triggers or args.compat)
    
    # Track environment backups
    environment_backups = {}
    return_code = 1  # Default to error
    
    try:
        # Track start time
        start_time = time.time()
        
        logger.info(f"Starting database tests at {datetime.now().strftime('%Y%m%d_%H%M%S')}")
        
        # Test configuration and database connectivity first
        dev_connected, test_connected = test_configuration()
        
        if not (dev_connected or test_connected):
            logger.error("Cannot connect to any database. Please check your database configuration.")
            return 1
        
        # Backup environments before testing unless explicitly skipped
        if not args.no_env_preserve:
            logger.info("Backing up environments before running tests")
            
            for env in ["dev", "test"]:
                if (env == 'dev' and dev_connected) or (env == 'test' and test_connected):
                    backup_file = backup_database(env)
                    if backup_file:
                        environment_backups[env] = backup_file
                    else:
                        logger.warning(f"Could not create backup of {env} environment")
        
        # Run selected tests
        if run_all or args.backup:
            test_backup_restore()
        
        if run_all or args.copy:
            test_database_copy()
        
        if run_all or args.triggers:
            test_trigger_management()
            
        if run_all or args.compat:
            test_backward_compatibility()
        
        # Always clean up unless explicitly skipped
        if not args.skip_cleanup:
            clean_up_test_artifacts()
        
        # Calculate execution time
        execution_time = time.time() - start_time
        
        # Display test results
        logger.info(results.summary())
        
        if results.failures:
            logger.error(results.failures_summary())
        
        logger.info(f"Tests completed in {execution_time:.2f} seconds")
        
        return_code = 0 if results.failed == 0 else 1
    
    except KeyboardInterrupt:
        logger.warning("Tests interrupted by user")
        return_code = 130
    except Exception as e:
        logger.error(f"Unhandled exception during tests: {e}", exc_info=True)
        return_code = 1
    
    finally:
        # Restore environments from backups unless explicitly skipped
        if environment_backups and not args.no_env_preserve:
            logger.info("\n" + "=" * 60)
            logger.info("RESTORING ENVIRONMENTS TO PRE-TEST STATE")
            logger.info("=" * 60)
            
            # Always restore even if tests failed
            if 'dev' in environment_backups and dev_connected:
                dev_backup = environment_backups['dev']
                logger.info(f"Restoring dev environment from {dev_backup}")
                success = restore_database('dev', dev_backup)
                if not success:
                    logger.error("Failed to restore dev environment")
            
            if 'test' in environment_backups and test_connected:
                test_backup = environment_backups['test']
                logger.info(f"Restoring test environment from {test_backup}")
                success = restore_database('test', test_backup)
                if not success:
                    logger.error("Failed to restore test environment")
    
    return return_code

if __name__ == "__main__":
    sys.exit(main())
                    CREATE TABLE _test_backup_table (
                        id SERIAL PRIMARY KEY,
                        test_column VARCHAR(50),
                        created_at TIMESTAMP DEFAULT NOW()
                    )
                """))
                results.add_pass("Backup - Create Test Table")
                
                # Insert test data
                connection.execute(text("INSERT INTO _test_backup_table (test_column) VALUES ('test_value')"))
                results.add_pass("Backup - Insert Test Data")
                
                # Verify data was inserted
                result = connection.execute(text("SELECT * FROM _test_backup_table"))
                rows = result.fetchall()
                if rows and len(rows) > 0:
                    logger.info(f"Test data verified: {rows}")
                    results.add_pass("Backup - Verify Test Data")
                else:
                    results.add_fail("Backup - Verify Test Data", "No data found in test table")
        
        # Create backup
        backup_file = backup_database('test')
        if not backup_file:
            results.add_fail("Backup - Create Backup", "Failed to create backup")
            return
        
        # Modify the table
        with engine.connect() as connection:
            with connection.begin():
                connection.execute(text("ALTER TABLE _test_backup_table ADD COLUMN extra VARCHAR(50)"))
                results.add_pass("Backup - Modify Table")
                
                # Verify column was added
                result = connection.execute(text("""
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_name = '_test_backup_table' AND column_name = 'extra'
                """))
                
                if result.fetchone():
                    logger.info("Column 'extra' was added successfully")
                    results.add_pass("Backup - Verify Column Added")
                else:
                    results.add_fail("Backup - Verify Column Added", "Column 'extra' was not added")
        
        # Restore from backup
        success = restore_database('test', backup_file)
        if not success:
            results.add_fail("Backup - Restore from Backup", "Failed to restore from backup")
            return
        
        # Verify the column was removed (schema restored to original state)
        with engine.connect() as connection:
            with connection.begin():
                result = connection.execute(text("""
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_name = '_test_backup_table' AND column_name = 'extra'
                """))
                
                if not result.fetchone():
                    logger.info("Column 'extra' was removed successfully during restore")
                    results.add_pass("Backup - Verify Restore")
                else:
                    results.add_fail("Backup - Verify Restore", "Column 'extra' was not removed during restore")
                
                # Clean up
                connection.execute(text("DROP TABLE IF EXISTS _test_backup_table"))
                logger.info("Cleaned up test table")
        
    except Exception as e:
        logger.error(f"Error in backup/restore test: {e}")
        results.add_fail("Backup/Restore Test", str(e))

def test_database_copy():
    """Test database copy functionality"""
    print_section_header("TESTING DATABASE COPY")
    
    # Ensure both databases are accessible
    if not test_database_connection('dev'):
        logger.error("Cannot access dev database, skipping database copy tests")
        results.add_skip("Database Copy Tests")
        return
    
    if not test_database_connection('test'):
        logger.error("Cannot access test database, skipping database copy tests")
        results.add_skip("Database Copy Tests")
        return
    
    # Create backup of both databases first
    dev_backup = backup_database('dev')
    test_backup = backup_database('test')
    
    if not dev_backup or not test_backup:
        results.add_fail("DB Copy - Create Backups", "Failed to create backups of dev or test database")
        return
    
    try:
        from sqlalchemy import create_engine, text
        
        dev_engine = get_db_engine('dev')
        test_engine = get_db_engine('test')
        
        # Create a test table in dev
        with dev_engine.connect() as connection:
            with connection.begin():
                # First check if the table already exists and drop it if it does
                connection.execute(text("DROP TABLE IF EXISTS _test_copy_table"))
                
                # Create the test table
                connection.execute(text("""
                    CREATE TABLE _test_copy_table (
                        id SERIAL PRIMARY KEY,
                        env VARCHAR(50),
                        value INTEGER
                    )
                """))
                results.add_pass("DB Copy - Create Dev Test Table")
                
                # Insert test data
                connection.execute(text("INSERT INTO _test_copy_table (env, value) VALUES ('dev', 100)"))
                results.add_pass("DB Copy - Insert Dev Test Data")
                
                # Verify data was inserted
                result = connection.execute(text("SELECT * FROM _test_copy_table"))
                rows = result.fetchall()
                if rows and len(rows) > 0:
                    logger.info(f"Dev test data verified: {rows}")
                    results.add_pass("DB Copy - Verify Dev Test Data")
                else:
                    results.add_fail("DB Copy - Verify Dev Test Data", "No data found in dev test table")
        
        # Export table schema and data
        table_schema = None
        table_data = None
        
        with dev_engine.connect() as connection:
            with connection.begin():
                # Get table schema
                columns_query = text("""
                    SELECT column_name, data_type, is_nullable
                    FROM information_schema.columns
                    WHERE table_name = '_test_copy_table'
                    ORDER BY ordinal_position
                """)
                columns = [dict(row._mapping) for row in connection.execute(columns_query)]
                
                # Get table data
                result = connection.execute(text("SELECT * FROM _test_copy_table"))
                rows = [dict(row._mapping) for row in result]
                
                # Store the schema and data
                table_schema = columns
                table_data = rows
        
        # Create table in test database
        with test_engine.connect() as connection:
            with connection.begin():
                # First check if the table already exists and drop it if it does
                connection.execute(text("DROP TABLE IF EXISTS _test_copy_table"))
                
                # Create the table based on the schema from dev
                column_defs = []
                for col in table_schema:
                    nullable = "" if col['is_nullable'] == 'YES' else "NOT NULL"
                    column_defs.append(f"{col['column_name']} {col['data_type']} {nullable}")
                
                create_sql = f"CREATE TABLE _test_copy_table ({', '.join(column_defs)})"
                connection.execute(text(create_sql))
                
                # Insert data
                for row in table_data:
                    columns = list(row.keys())
                    placeholders = [f":{col}" for col in columns]
                    
                    insert_sql = f"INSERT INTO _test_copy_table ({', '.join(columns)}) VALUES ({', '.join(placeholders)})"
                    connection.execute(text(insert_sql), row)
                
                results.add_pass("DB Copy - Copy Table to Test")
                
                # Verify data was inserted
                result = connection.execute(text("SELECT * FROM _test_copy_table"))
                rows = result.fetchall()
                if rows and len(rows) > 0:
                    logger.info(f"Test database data verified: {rows}")
                    results.add_pass("DB Copy - Verify Test Data")
                else:
                    results.add_fail("DB Copy - Verify Test Data", "No data found in test database table")
                
                # Modify data in test database
                connection.execute(text("UPDATE _test_copy_table SET env = 'test', value = 200"))
                results.add_pass("DB Copy - Modify Test Data")
        
        # Test schema-only copy
        # First modify dev table schema
        with dev_engine.connect() as connection:
            with connection.begin():
                connection.execute(text("ALTER TABLE _test_copy_table ADD COLUMN dev_only VARCHAR(50) DEFAULT 'dev_feature'"))
                results.add_pass("DB Copy - Modify Dev Schema")
                
                # Export updated schema
                columns_query = text("""
                    SELECT column_name, data_type, is_nullable
                    FROM information_schema.columns
                    WHERE table_name = '_test_copy_table'
                    ORDER BY ordinal_position
                """)
                updated_schema = [dict(row._mapping) for row in connection.execute(columns_query)]
        
        # Save current data from test table before schema update
        test_data = None
        with test_engine.connect() as connection:
            with connection.begin():
                # Get current data
                result = connection.execute(text("SELECT * FROM _test_copy_table"))
                test_data = [dict(row._mapping) for row in result]
        
        # Update schema in test database but preserve data
        with test_engine.connect() as connection:
            with connection.begin():
                # Drop existing table (but we have the data saved)
                connection.execute(text("DROP TABLE IF EXISTS _test_copy_table"))
                
                # Recreate with new schema
                column_defs = []
                for col in updated_schema:
                    nullable = "" if col['is_nullable'] == 'YES' else "NOT NULL"
                    column_defs.append(f"{col['column_name']} {col['data_type']} {nullable}")
                
                create_sql = f"CREATE TABLE _test_copy_table ({', '.join(column_defs)})"
                connection.execute(text(create_sql))
                
                # Insert saved data (matching just the columns we have)
                if test_data:
                    for row in test_data:
                        # Make sure we only insert columns that exist in both old and new schema
                        columns = [c for c in row.keys() if c in [col['column_name'] for col in updated_schema]]
                        if not columns:
                            continue
                            
                        placeholders = [f":{col}" for col in columns]
                        
                        # Create a filtered row with just the columns we're inserting
                        filtered_row = {col: row[col] for col in columns}
                        
                        insert_sql = f"INSERT INTO _test_copy_table ({', '.join(columns)}) VALUES ({', '.join(placeholders)})"
                        connection.execute(text(insert_sql), filtered_row)
                
                results.add_pass("DB Copy - Schema-Only Copy")
                
                # Verify schema was updated
                col_check = connection.execute(text("""
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_name = '_test_copy_table' AND column_name = 'dev_only'
                """))
                
                if col_check.fetchone():
                    logger.info("Column 'dev_only' was added successfully during schema copy")
                    results.add_pass("DB Copy - Verify Schema Update")
                else:
                    results.add_fail("DB Copy - Verify Schema Update", "Column 'dev_only' was not added during schema copy")
                
                # Check data was preserved (at least partially)
                result = connection.execute(text("SELECT env, value FROM _test_copy_table"))
                rows = result.fetchall()
                if rows and len(rows) > 0:
                    logger.info("Data preserved during schema copy")
                    results.add_pass("DB Copy - Verify Data Preserved")
                else:
                    results.add_fail("DB Copy - Verify Data Preserved", "Data was not preserved during schema copy")
        
        # Clean up
        with dev_engine.connect() as connection:
            with connection.begin():
                connection.execute(text("DROP TABLE IF EXISTS _test_copy_table"))
                logger.info("Cleaned up test table in dev database")
        
        with test_engine.connect() as connection:
            with connection.begin():
                connection.execute(text("DROP TABLE IF EXISTS _test_copy_table"))
                logger.info("Cleaned up test table in test database")
        
    except Exception as e:
        logger.error(f"Error in database copy test: {e}")
        results.add_fail("Database Copy Test", str(e))
    
    # Restore databases to their original state
    logger.info("Restoring databases to original state")
    restore_database('dev', dev_backup)
    restore_database('test', test_backup)

def test_trigger_management():
    """Test database trigger management"""
    print_section_header("TESTING TRIGGER MANAGEMENT")
    
    # Ensure test database is accessible
    if not test_database_connection('test'):
        logger.error("Cannot access test database, skipping trigger tests")
        results.add_skip("Trigger Management Tests")
        return
    
    # Create backup of test database first
    test_backup = backup_database('test')
    if not test_backup:
        results.add_fail("Triggers - Create Backup", "Failed to create backup of test database")
        return
    
    try:
        from sqlalchemy import create_engine, text
        
        engine = get_db_engine('test')
        
        # Create a test table
        with engine.connect() as connection:
            with connection.begin():
                # First check if the table already exists and drop it if it does
                connection.execute(text("DROP TABLE IF EXISTS _test_trigger_table"))
                
                # Create the test table
                connection.execute(text("""